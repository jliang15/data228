{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d935343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3111213\n",
      "#YY                0\n",
      "MM                 0\n",
      "DD                 0\n",
      "hh                 0\n",
      "mm                 0\n",
      "WDIR          992810\n",
      "WSPD          986877\n",
      "GST          1201764\n",
      "WVHT         2829788\n",
      "DPD          2829788\n",
      "APD          2829787\n",
      "MWD          2852039\n",
      "PRES             179\n",
      "ATMP          466734\n",
      "WTMP         1634345\n",
      "DEWP         2312132\n",
      "VIS          2425237\n",
      "TIDE         3111213\n",
      "timestamp          0\n",
      "year               0\n",
      "month              0\n",
      "day                0\n",
      "hour               0\n",
      "dtype: int64\n",
      "          #YY  MM  DD  hh  mm   WDIR  WSPD   GST    PRES  ATMP  \\\n",
      "0        2014   1   1   0  50  313.0   5.7   6.8  1022.6  10.2   \n",
      "1        2014   1   1   1  50  315.0   5.0   6.1  1022.4  10.2   \n",
      "2        2014   1   1   2  50  319.0   5.6   6.6  1022.3  10.3   \n",
      "3        2014   1   1   3  50  318.0   4.6   5.7  1022.4  10.4   \n",
      "4        2014   1   1   4  50  314.0   4.2   5.1  1022.5  10.5   \n",
      "...       ...  ..  ..  ..  ..    ...   ...   ...     ...   ...   \n",
      "1677825  2022  12  31  23  36  317.0   5.8   8.6  1002.8  10.9   \n",
      "1677826  2022  12  31  23  42  320.0   7.1   8.8  1002.9  10.8   \n",
      "1677827  2022  12  31  23  48  312.0   7.2  10.6  1003.0  10.7   \n",
      "1677828  2022  12  31  23  54  317.0   7.9  11.1  1003.1  10.9   \n",
      "1677829  2016   7  18  14  53  187.0   2.2   2.2  1018.0  13.6   \n",
      "\n",
      "                  timestamp BEGIN_DATE  BEGIN_TIME EVENT_TYPE  \n",
      "0       2014-01-01 00:00:00        NaN         NaN         no  \n",
      "1       2014-01-01 01:00:00        NaN         NaN         no  \n",
      "2       2014-01-01 02:00:00        NaN         NaN         no  \n",
      "3       2014-01-01 03:00:00        NaN         NaN         no  \n",
      "4       2014-01-01 04:00:00        NaN         NaN         no  \n",
      "...                     ...        ...         ...        ...  \n",
      "1677825 2022-12-31 23:00:00        NaN         NaN         no  \n",
      "1677826 2022-12-31 23:00:00        NaN         NaN         no  \n",
      "1677827 2022-12-31 23:00:00        NaN         NaN         no  \n",
      "1677828 2022-12-31 23:00:00        NaN         NaN         no  \n",
      "1677829 2016-07-18 14:00:00        NaN         NaN         no  \n",
      "\n",
      "[1677830 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "feature_file_paths = [\n",
    "    'dataset/46013h2014.txt', 'dataset/46013h2015.txt', 'dataset/46013h2016.txt', 'dataset/46013h2017.txt', 'dataset/46013h2018.txt', 'dataset/46013h2019.txt', 'dataset/46013h2021.txt', 'dataset/46013h2022.txt',\n",
    "    'dataset/46026h2014.txt', 'dataset/46026h2015.txt', 'dataset/46026h2016.txt', 'dataset/46026h2017.txt', 'dataset/46026h2018.txt', 'dataset/46026h2019.txt', 'dataset/46026h2020.txt', 'dataset/46026h2021.txt', 'dataset/46026h2022.txt',\n",
    "    'dataset/46237h2014.txt', 'dataset/46237h2015.txt', 'dataset/46237h2016.txt', 'dataset/46237h2017.txt', 'dataset/46237h2018.txt', 'dataset/46237h2019.txt', 'dataset/46237h2020.txt', 'dataset/46237h2021.txt', 'dataset/46237h2022.txt',\n",
    "    'dataset/ftpc1h2014.txt', 'dataset/ftpc1h2015.txt', 'dataset/ftpc1h2016.txt', 'dataset/ftpc1h2017.txt', 'dataset/ftpc1h2018.txt', 'dataset/ftpc1h2019.txt', 'dataset/ftpc1h2020.txt', 'dataset/ftpc1h2021.txt', 'dataset/ftpc1h2022.txt',\n",
    "    'dataset/pxoc1h2014.txt', 'dataset/pxoc1h2015.txt', 'dataset/pxoc1h2016.txt', 'dataset/pxoc1h2017.txt', 'dataset/pxoc1h2018.txt', 'dataset/pxoc1h2019.txt', 'dataset/pxoc1h2020.txt', 'dataset/pxoc1h2021.txt', 'dataset/pxoc1h2022.txt',\n",
    "    'dataset/pxsc1h2014.txt', 'dataset/pxsc1h2015.txt', 'dataset/pxsc1h2016.txt', 'dataset/pxsc1h2017.txt', 'dataset/pxsc1h2018.txt', 'dataset/pxsc1h2019.txt', 'dataset/pxsc1h2020.txt', 'dataset/pxsc1h2021.txt', 'dataset/pxsc1h2022.txt',\n",
    "    'dataset/tibc1h2015.txt', 'dataset/tibc1h2016.txt', 'dataset/tibc1h2017.txt', 'dataset/tibc1h2018.txt', 'dataset/tibc1h2019.txt', 'dataset/tibc1h2020.txt', 'dataset/tibc1h2021.txt', 'dataset/tibc1h2022.txt'\n",
    "]\n",
    "\n",
    "\n",
    "# dataframe for all feature data\n",
    "all_feature_data = pd.DataFrame()\n",
    "\n",
    "# load all feature files and create timestamp column\n",
    "for feature_file_path in feature_file_paths:\n",
    "    feature_data = pd.read_csv(feature_file_path, delim_whitespace=True, skiprows=[1])  \n",
    "    feature_data['timestamp'] = pd.to_datetime(feature_data[['#YY', 'MM', 'DD', 'hh']].astype(str).agg(' '.join, axis=1), format='%Y %m %d %H')\n",
    "    feature_data['year'] = feature_data['timestamp'].dt.year\n",
    "    feature_data['month'] = feature_data['timestamp'].dt.month\n",
    "    feature_data['day'] = feature_data['timestamp'].dt.day\n",
    "    feature_data['hour'] = feature_data['timestamp'].dt.hour\n",
    "    all_feature_data = pd.concat([all_feature_data, feature_data], axis=0, ignore_index=True)\n",
    "\n",
    "    \n",
    "print(all_feature_data.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Define the missing value patterns\n",
    "missing_patterns = [99.00, 999, 999.0, 99.0]\n",
    "\n",
    "# Loop through each column and replace each pattern with NaN\n",
    "for column in all_feature_data.columns:\n",
    "    for pattern in missing_patterns:\n",
    "        all_feature_data[column] = all_feature_data[column].replace(to_replace=pattern, value=np.nan, regex=True)\n",
    "\n",
    "missing_data = all_feature_data.isnull().sum()\n",
    "\n",
    "print(missing_data)\n",
    "    \n",
    "# Setting threshold for excessive missing values (e.g., 60%)\n",
    "threshold = 0.6 * len(all_feature_data)\n",
    "\n",
    "# Drop columns with missing values greater than the threshold\n",
    "all_feature_data.dropna(axis=1, thresh=threshold, inplace=True)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "all_feature_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# target file path and read the file\n",
    "target_file_path = 'dataset/storm_data_search_results.csv' \n",
    "target_data = pd.read_csv(target_file_path, sep=',') \n",
    "\n",
    "# the columns below are useful for target\n",
    "selected_columns = ['BEGIN_DATE', 'BEGIN_TIME', 'EVENT_TYPE']\n",
    "target_data = target_data[selected_columns]\n",
    "\n",
    "# convert event type values to yes\n",
    "target_data['EVENT_TYPE'] = 'yes'\n",
    "\n",
    "\n",
    "\n",
    "# create timestamp column for the target file\n",
    "target_data['timestamp'] = pd.to_datetime(target_data[['BEGIN_DATE', 'BEGIN_TIME']].astype(str).agg(' '.join, axis=1), format='%m/%d/%Y %H%M')\n",
    "target_data['year'] = target_data['timestamp'].dt.year\n",
    "target_data['month'] = target_data['timestamp'].dt.month\n",
    "target_data['day'] = target_data['timestamp'].dt.day\n",
    "target_data['hour'] = target_data['timestamp'].dt.hour\n",
    "\n",
    "#print(target_data['hour'])\n",
    "\n",
    "#target_data['EVENT_TYPE'].fillna('no', inplace=True)\n",
    "\n",
    "\n",
    "'''label_encoder = LabelEncoder()\n",
    "\n",
    "for column in target_data.select_dtypes(include=['object']).columns:\n",
    "    target_data[column] = label_encoder.fit_transform(target_data[column])'''\n",
    "\n",
    "\n",
    "# combine feature and target files\n",
    "all_data = pd.merge(all_feature_data, target_data, how='left', on='timestamp')\n",
    "all_data['EVENT_TYPE'].fillna('no', inplace=True)\n",
    "#all_data['EVENT_TYPE'] = all_data['EVENT_TYPE'].replace(['Thunderstorm Wind', 'Storm Surge/Tide'], 'yes')\n",
    "'''all_data['EVENT_TYPE'] = all_data['EVENT_TYPE'].replace({'Flood': 'yes', 'Hail': 'yes', 'Strong Wind': 'yes', 'Rip Current': 'yes',\n",
    "                                                          'Wildfire': 'yes', 'Thunderstorm Wind': 'yes', 'Dense Smoke': 'yes',\n",
    "                                                          'Excessive Heat': 'yes', 'Storm Surge/Tide': 'yes', 'Coastal Flood': 'yes'})'''\n",
    "\n",
    "all_data = all_data.drop(['year_x', 'month_x', 'day_x', 'hour_x', 'year_y', 'month_y', 'day_y', 'hour_y'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "print(all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e4221015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n"
     ]
    }
   ],
   "source": [
    "# drop columns that is not useful for modeling. Define feature and target\n",
    "X = all_data.drop(['EVENT_TYPE','timestamp','BEGIN_DATE', 'BEGIN_TIME','#YY','MM',  'DD',  'hh',  'mm'], axis=1)  \n",
    "y = all_data['EVENT_TYPE'] \n",
    "\n",
    "yes_count = all_data['EVENT_TYPE'].value_counts().get('yes', 0)\n",
    "print(yes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "581ac2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlottezhang/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT_TYPE\n",
      "no     796\n",
      "yes    796\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlottezhang/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/charlottezhang/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8504794883867853\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#start undersampling\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# look for the minority class\n",
    "minority_class_label = train_data['EVENT_TYPE'].value_counts().idxmin()\n",
    "\n",
    "# apply random undersampling\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto')\n",
    "X_resampled, y_resampled = undersampler.fit_resample(train_data.drop('EVENT_TYPE', axis=1), train_data['EVENT_TYPE'])\n",
    "print(y_resampled.value_counts())\n",
    "\n",
    "# train using undersampled data\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# make predictions\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# make evaluation \n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f'Accuracy on test set: {accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925932d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
